{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# phase 1 - data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## original data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD\n",
      "                review_id                 user_id             business_id  \\\n",
      "0  KU_O5udG6zpxOg-VcAEodg  mh_-eMZ6K5RLWhZyISBhwA  XQfwVwDr-v0ZS3_CbbE5Xw   \n",
      "1  BiTunyQ73aT9WBnpR9DZGw  OyoGAe7OKpv6SyGZT5g77Q  7ATYjTIgM3jUlt4UM3IypQ   \n",
      "2  saUsX_uimxRlCVr67Z4Jig  8g_iMtfSiwikVnbP2etR0A  YjUWPpI6HXG530lwP-fb2A   \n",
      "3  AqPFMleE6RsU23_auESxiA  _7bHUi9Uuf5__HHc_Q8guQ  kxX2SOes4o-D3ZQBkiMRfA   \n",
      "4  Sx8TMOWLNuJBWer-0pcmoA  bcjbaE6dDog4jkNY91ncLQ  e4Vwtrqf-wpJfwesgvdgxQ   \n",
      "\n",
      "   stars  useful  funny  cool  \\\n",
      "0      3       0      0     0   \n",
      "1      5       1      0     1   \n",
      "2      3       0      0     0   \n",
      "3      5       1      0     1   \n",
      "4      4       1      0     1   \n",
      "\n",
      "                                                text                date  \n",
      "0  If you decide to eat here, just be aware it is... 2018-07-07 22:09:11  \n",
      "1  I've taken a lot of spin classes over the year... 2012-01-03 15:28:18  \n",
      "2  Family diner. Had the buffet. Eclectic assortm... 2014-02-05 20:30:30  \n",
      "3  Wow!  Yummy, different,  delicious.   Our favo... 2015-01-04 00:01:03  \n",
      "4  Cute interior and owner (?) gave us tour of up... 2017-01-14 20:54:15  \n",
      "\n",
      "COLUMNS\n",
      "Index(['review_id', 'user_id', 'business_id', 'stars', 'useful', 'funny',\n",
      "       'cool', 'text', 'date'],\n",
      "      dtype='object')\n",
      "\n",
      "INFO\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   review_id    100000 non-null  object        \n",
      " 1   user_id      100000 non-null  object        \n",
      " 2   business_id  100000 non-null  object        \n",
      " 3   stars        100000 non-null  int64         \n",
      " 4   useful       100000 non-null  int64         \n",
      " 5   funny        100000 non-null  int64         \n",
      " 6   cool         100000 non-null  int64         \n",
      " 7   text         100000 non-null  object        \n",
      " 8   date         100000 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(4), object(4)\n",
      "memory usage: 6.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# select first 100 000 records, check first records, columns and missing information\n",
    "\n",
    "chunks = pd.read_json(\"yelp_academic_dataset_review.json\", lines=True, chunksize=100000)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(\"HEAD\")\n",
    "    print(chunk.head())\n",
    "    print(\"\\nCOLUMNS\")\n",
    "    print(chunk.columns)\n",
    "    print(\"\\nINFO\")\n",
    "    print(chunk.info())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS\n",
      "Index(['stars', 'text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# drop irrelevant columns, process text and check for missing values\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text) # remove special characters\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.discard('not')\n",
    "    filtered_tokens = [token for token in tokens if token.casefold() not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "irrelevant_columns = [\"review_id\", \"user_id\", \"business_id\", \"useful\", \"funny\", \"cool\", \"date\"]\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk = chunk.drop(columns=irrelevant_columns)\n",
    "    chunk[\"text\"] = chunk[\"text\"].apply(clean_text)\n",
    "    with open(\"yelp_reviews_cleaned.csv\", mode=\"a\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        chunk.to_csv(file, index=False, header=(i==0))\n",
    "\n",
    "    print(\"COLUMNS\")\n",
    "    print(chunk.columns)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75220    fried pickle appetizer lunch fried green tomat...\n",
       "48955    Went great review yelpit everything expected f...\n",
       "44966    LOVE NEW PLACE Ate pickled vegetable chicken w...\n",
       "13568    wife three time time look forward trying somet...\n",
       "92727    first time visiting Libertine resulted really ...\n",
       "                               ...                        \n",
       "6265     hand best florida like barbqyou love place goi...\n",
       "54886    Damn good steak salad perfect Bloody Marys bac...\n",
       "76820    lunch afternoon wife Good food great service t...\n",
       "860      Food cold Waitress said one person working bac...\n",
       "15795    One best prime rib dip Ive ever hand attached ...\n",
       "Name: text, Length: 80000, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"yelp_reviews_cleaned.csv\")\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"text\"], df[\"stars\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "type(train_texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
