{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# phase 2 - convert raw text into numeric representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bow_to_file(input_filename: str, output_filename: str) -> None:\n",
    "    # read data from input file\n",
    "    data = pd.read_csv(input_filename)\n",
    "\n",
    "    # remove rows where text is NaN\n",
    "    data = data.dropna(subset=[\"text\"]) \n",
    "\n",
    "    # extract corpus and indexes\n",
    "    corpus = data[\"text\"].values\n",
    "    indexes = data[\"index\"].values\n",
    "    \n",
    "    # create BoW model\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    bow_data = []\n",
    "    for i, doc_index in enumerate(indexes):\n",
    "        # get indices of nonzero word counts\n",
    "        nonzero_indices = X[i].nonzero()[1]  \n",
    "        # get the actual nonzero word counts\n",
    "        nonzero_values = X[i].data  \n",
    "        # create a sparse dictionary of word index -> count for the document\n",
    "        sparse_representation = {int(idx): int(val) for idx, val in zip(nonzero_indices, nonzero_values)}\n",
    "        # append the representation along with the document's original index\n",
    "        bow_data.append({\"index\": doc_index, \"text\": sparse_representation})\n",
    "\n",
    "    # save the processed BoW data to the output CSV file\n",
    "    pd.DataFrame(bow_data).to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_bow_to_file(\"../data/80_20/test_texts.csv\", \"../data/80_20/test_texts_bow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_bow_to_file(\"../data/80_20/train_texts.csv\", \"../data/80_20/train_texts_bow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_bow_to_file(\"../data/70_30/test_texts.csv\", \"../data/70_30/test_texts_bow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_bow_to_file(\"../data/70_30/train_texts.csv\", \"../data/70_30/train_texts_bow.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_word2vec_to_file(input_filename: str, output_filename: str) -> None:\n",
    "    # read data from input file\n",
    "    data = pd.read_csv(input_filename)\n",
    "    \n",
    "    # remove rows where text is NaN\n",
    "    data = data.dropna(subset=[\"text\"])\n",
    "\n",
    "    # extract corpus and indexes\n",
    "    corpus = data[\"text\"].values\n",
    "    indexes = data[\"index\"].values\n",
    "\n",
    "    # convert corpus into a list of lists of tokens\n",
    "    corpus_tokenized = [word_tokenize(sentence) for sentence in corpus]\n",
    "\n",
    "    # train the Word2Vec model and save it\n",
    "    model = Word2Vec(sentences=corpus_tokenized, vector_size=50, window=5, min_count=5, workers=4, epochs=5)\n",
    "    model.save(\"../models/word2vec_model\")\n",
    "    \n",
    "    word2vec_data = []\n",
    "    for i, doc_index in enumerate(indexes):\n",
    "        review_as_word2vec = []\n",
    "        for token in corpus_tokenized[i]:\n",
    "            if token in model.wv:\n",
    "                # if the word is in the vocabulary, get its vector\n",
    "                review_as_word2vec.append(model.wv[token].tolist())\n",
    "            else:\n",
    "                # if not in vocabulary, represent it as a zero vector\n",
    "                review_as_word2vec.append([0.0] * model.vector_size)\n",
    "        # append the index and vectorized review to the output list\n",
    "        word2vec_data.append({\"index\": doc_index, \"text\": review_as_word2vec})\n",
    "\n",
    "    # save the processed w2v data to the output CSV file\n",
    "    pd.DataFrame(word2vec_data).to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_word2vec_to_file(\"../data/80_20/test_texts.csv\", \"../data/80_20/test_texts_w2v.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../models/word2vec_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Maciej\\Desktop\\sem6\\UCZENIE MASZYNOWE\\sentiment_analysis\\.venv\\lib\\site-packages\\gensim\\utils.py:763\u001b[0m, in \u001b[0;36mSaveLoad.save\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 763\u001b[0m     \u001b[43m_pickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    764\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: file must have a 'write' attribute",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mapply_word2vec_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/80_20/train_texts.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/80_20/train_texts_w2v.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 17\u001b[0m, in \u001b[0;36mapply_word2vec_to_file\u001b[1;34m(input_filename, output_filename)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# train the Word2Vec model and save it\u001b[39;00m\n\u001b[0;32m     16\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec(sentences\u001b[38;5;241m=\u001b[39mcorpus_tokenized, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../models/word2vec_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m word2vec_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, doc_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(indexes):\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\Desktop\\sem6\\UCZENIE MASZYNOWE\\sentiment_analysis\\.venv\\lib\\site-packages\\gensim\\models\\word2vec.py:1923\u001b[0m, in \u001b[0;36mWord2Vec.save\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1913\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save the model.\u001b[39;00m\n\u001b[0;32m   1914\u001b[0m \u001b[38;5;124;03m    This saved model can be loaded again using :func:`~gensim.models.word2vec.Word2Vec.load`, which supports\u001b[39;00m\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;124;03m    online training and getting vectors for vocabulary words.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1921\u001b[0m \n\u001b[0;32m   1922\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1923\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Word2Vec, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\Desktop\\sem6\\UCZENIE MASZYNOWE\\sentiment_analysis\\.venv\\lib\\site-packages\\gensim\\utils.py:766\u001b[0m, in \u001b[0;36mSaveLoad.save\u001b[1;34m(self, fname_or_handle, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    764\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m object\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:  \u001b[38;5;66;03m# `fname_or_handle` does not have write attribute\u001b[39;00m\n\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_smart_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparately\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep_limit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\Desktop\\sem6\\UCZENIE MASZYNOWE\\sentiment_analysis\\.venv\\lib\\site-packages\\gensim\\utils.py:610\u001b[0m, in \u001b[0;36mSaveLoad._smart_save\u001b[1;34m(self, fname, separately, sep_limit, ignore, pickle_protocol)\u001b[0m\n\u001b[0;32m    606\u001b[0m restores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_specials(\n\u001b[0;32m    607\u001b[0m     fname, separately, sep_limit, ignore, pickle_protocol, compress, subname,\n\u001b[0;32m    608\u001b[0m )\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    612\u001b[0m     \u001b[38;5;66;03m# restore attribs handled specially\u001b[39;00m\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj, asides \u001b[38;5;129;01min\u001b[39;00m restores:\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\Desktop\\sem6\\UCZENIE MASZYNOWE\\sentiment_analysis\\.venv\\lib\\site-packages\\gensim\\utils.py:1441\u001b[0m, in \u001b[0;36mpickle\u001b[1;34m(obj, fname, protocol)\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpickle\u001b[39m(obj, fname, protocol\u001b[38;5;241m=\u001b[39mPICKLE_PROTOCOL):\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pickle object `obj` to file `fname`, using smart_open so that `fname` can be on S3, HDFS, compressed etc.\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m \n\u001b[0;32m   1431\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \n\u001b[0;32m   1440\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1441\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fout:  \u001b[38;5;66;03m# 'b' for binary, needed on Windows\u001b[39;00m\n\u001b[0;32m   1442\u001b[0m         _pickle\u001b[38;5;241m.\u001b[39mdump(obj, fout, protocol\u001b[38;5;241m=\u001b[39mprotocol)\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\Desktop\\sem6\\UCZENIE MASZYNOWE\\sentiment_analysis\\.venv\\lib\\site-packages\\smart_open\\smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transport_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    175\u001b[0m     transport_params \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 177\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_shortcut_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fobj\n",
      "File \u001b[1;32mc:\\Users\\Maciej\\Desktop\\sem6\\UCZENIE MASZYNOWE\\sentiment_analysis\\.venv\\lib\\site-packages\\smart_open\\smart_open_lib.py:375\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m    373\u001b[0m     open_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m errors\n\u001b[1;32m--> 375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[38;5;241m=\u001b[39mbuffering, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopen_kwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../models/word2vec_model'"
     ]
    }
   ],
   "source": [
    "apply_word2vec_to_file(\"../data/80_20/train_texts.csv\", \"../data/80_20/train_texts_w2v.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_word2vec_to_file(\"../data/70_30/test_texts.csv\", \"../data/70_30/test_texts_w2v.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_word2vec_to_file(\"../data/70_30/train_texts.csv\", \"../data/70_30/train_texts_w2v.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
