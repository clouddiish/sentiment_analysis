{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08480a3b",
   "metadata": {},
   "source": [
    "Importujemy potrzebne biblioteki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49419976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importujemy biblioteki\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import ast\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3752d991",
   "metadata": {},
   "source": [
    "Evaluacja danych dla wartości 70_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "814fb68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model i vectorizer zostały wczytane z jednego pliku.\n",
      "Wczytano dane testowe: 29999 rekordów.\n",
      "Rozkład klas w zbiorze testowym:\n",
      "stars\n",
      "5    0.444867\n",
      "4    0.254133\n",
      "3    0.113767\n",
      "1    0.109133\n",
      "2    0.078100\n",
      "Name: proportion, dtype: float64\n",
      "Dane testowe zostały przekształcone.\n",
      "Predykcja zakończona.\n",
      "Accuracy: 0.08030267675589187\n",
      "Precision: 0.24819907717051423\n",
      "Recall: 0.08030267675589187\n",
      "F1 Score: 0.019360094545052612\n",
      "Wyniki ewaluacji zapisane w folderze: ../evaluation/eval_BoW_70_30\n"
     ]
    }
   ],
   "source": [
    "# Ścieżki\n",
    "test_texts_path = '../data/70_30/test_texts_bow.csv'\n",
    "test_labels_path = '../data/70_30/test_labels.csv'\n",
    "model_path = '../models/BoW_70_30.pkl'\n",
    "evaluation_dir = '../evaluation/eval_BoW_70_30'\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "# Wczytujemy model i vectorizer z jednego pliku\n",
    "with open(model_path, 'rb') as file:\n",
    "    model_package = pickle.load(file)\n",
    "\n",
    "vectorizer = model_package['vectorizer']\n",
    "model = model_package['model']\n",
    "\n",
    "print(\"Model i vectorizer zostały wczytane z jednego pliku.\")\n",
    "\n",
    "# Wczytujemy dane testowe\n",
    "X_test_raw = pd.read_csv(test_texts_path, index_col=0)\n",
    "y_test = pd.read_csv(test_labels_path, index_col=0).squeeze()\n",
    "\n",
    "print(f\"Wczytano dane testowe: {X_test_raw.shape[0]} rekordów.\")\n",
    "\n",
    "# Rozkład klas w zbiorze testowym\n",
    "print(\"Rozkład klas w zbiorze testowym:\")\n",
    "class_distribution = y_test.value_counts(normalize=True)\n",
    "print(class_distribution)\n",
    "\n",
    "# Zapisujemy rozkład klas\n",
    "class_distribution.to_csv(os.path.join(evaluation_dir, 'class_distribution_test_70_30.csv'))\n",
    "\n",
    "# Wyrównujemy indeksy\n",
    "X_test_raw, y_test = X_test_raw.align(y_test, join='inner', axis=0)\n",
    "\n",
    "# Parsujemy dane testowe\n",
    "X_test_dicts = X_test_raw.iloc[:, 0].apply(ast.literal_eval)\n",
    "\n",
    "# Transformujemy dane testowe\n",
    "X_test = vectorizer.transform(X_test_dicts)\n",
    "\n",
    "print(\"Dane testowe zostały przekształcone.\")\n",
    "\n",
    "# Predykcja\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Predykcja zakończona.\")\n",
    "\n",
    "# Obliczamy metryki\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Tworzymy podsumowanie metryk\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'metric': ['accuracy', 'precision', 'recall', 'f1_score'],\n",
    "    'value': [accuracy, precision, recall, f1]\n",
    "})\n",
    "\n",
    "# Zapisujemy podsumowanie do pliku CSV\n",
    "metrics_summary.to_csv(os.path.join(evaluation_dir, 'metrics_summary_70_30.csv'), index=False)\n",
    "\n",
    "# Confusion matrix zapisujemy osobno\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix)\n",
    "conf_matrix_df.to_csv(os.path.join(evaluation_dir, 'confusion_matrix_70_30.csv'), index=False)\n",
    "\n",
    "print(f\"Wyniki ewaluacji zapisane w folderze: {evaluation_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7961aed7",
   "metadata": {},
   "source": [
    "Evaluacja danych dla wartości 80_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c353f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model i vectorizer zostały wczytane z jednego pliku.\n",
      "Wczytano dane testowe: 19999 rekordów.\n",
      "Rozkład klas w zbiorze testowym:\n",
      "stars\n",
      "5    0.44355\n",
      "4    0.25460\n",
      "3    0.11375\n",
      "1    0.10950\n",
      "2    0.07860\n",
      "Name: proportion, dtype: float64\n",
      "Dane testowe zostały przekształcone.\n",
      "Predykcja zakończona.\n",
      "Accuracy: 0.07960398019900995\n",
      "Precision: 0.3470461796436442\n",
      "Recall: 0.07960398019900995\n",
      "F1 Score: 0.020275194797392305\n",
      "Wyniki ewaluacji zapisane w folderze: ../evaluation/eval_BoW_80_20\n"
     ]
    }
   ],
   "source": [
    "# Ścieżki\n",
    "test_texts_path = '../data/80_20/test_texts_bow.csv'\n",
    "test_labels_path = '../data/80_20/test_labels.csv'\n",
    "model_path = '../models/BoW_80_20.pkl'\n",
    "evaluation_dir = '../evaluation/eval_BoW_80_20'\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "# Wczytujemy model i vectorizer z jednego pliku\n",
    "with open(model_path, 'rb') as file:\n",
    "    model_package = pickle.load(file)\n",
    "\n",
    "vectorizer = model_package['vectorizer']\n",
    "model = model_package['model']\n",
    "\n",
    "print(\"Model i vectorizer zostały wczytane z jednego pliku.\")\n",
    "\n",
    "# Wczytujemy dane testowe\n",
    "X_test_raw = pd.read_csv(test_texts_path, index_col=0)\n",
    "y_test = pd.read_csv(test_labels_path, index_col=0).squeeze()\n",
    "\n",
    "print(f\"Wczytano dane testowe: {X_test_raw.shape[0]} rekordów.\")\n",
    "\n",
    "# Rozkład klas w zbiorze testowym\n",
    "print(\"Rozkład klas w zbiorze testowym:\")\n",
    "class_distribution = y_test.value_counts(normalize=True)\n",
    "print(class_distribution)\n",
    "\n",
    "# Zapisujemy rozkład klas\n",
    "class_distribution.to_csv(os.path.join(evaluation_dir, 'class_distribution_test_80_20.csv'))\n",
    "\n",
    "# Wyrównujemy indeksy\n",
    "X_test_raw, y_test = X_test_raw.align(y_test, join='inner', axis=0)\n",
    "\n",
    "# Parsujemy dane testowe\n",
    "X_test_dicts = X_test_raw.iloc[:, 0].apply(ast.literal_eval)\n",
    "\n",
    "# Transformujemy dane testowe\n",
    "X_test = vectorizer.transform(X_test_dicts)\n",
    "\n",
    "print(\"Dane testowe zostały przekształcone.\")\n",
    "\n",
    "# Predykcja\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Predykcja zakończona.\")\n",
    "\n",
    "# Obliczamy metryki\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Tworzymy podsumowanie metryk\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'metric': ['accuracy', 'precision', 'recall', 'f1_score'],\n",
    "    'value': [accuracy, precision, recall, f1]\n",
    "})\n",
    "\n",
    "# Zapisujemy podsumowanie do pliku CSV\n",
    "metrics_summary.to_csv(os.path.join(evaluation_dir, 'metrics_summary_80_20.csv'), index=False)\n",
    "\n",
    "# Confusion matrix zapisujemy osobno\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix)\n",
    "conf_matrix_df.to_csv(os.path.join(evaluation_dir, 'confusion_matrix_80_20.csv'), index=False)\n",
    "\n",
    "print(f\"Wyniki ewaluacji zapisane w folderze: {evaluation_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d6e65",
   "metadata": {},
   "source": [
    "Wnioski dlaczego mimo zbliżonych danych precyzja jest taka mała\n",
    "\n",
    "1.Sama natura danych model to Bag-of-Words + Naive Bayes.\n",
    "\n",
    "Bag-of-Words: traktuje słowa jak worek tokenów, bez kontekstu.\n",
    "Naive Bayes: zakłada niezależność cech, co dla języka naturalnego nie jest prawdziwe (w zdaniach znaczenie tworzy kontekst słów razem).\n",
    "Więc model nie \"rozumie\", jak słowa wpływają na znaczenie oceny.\n",
    "\n",
    "2.Problem wieloklasowości (multiclass)\n",
    "\n",
    "Masz pięć klas (oceny 1–5 gwiazdek). Model ma znacznie trudniejsze zadanie niż klasyczne binary classification (np pozytywne/negatywne).\n",
    "\n",
    "3.W Twoim zbiorze dane są już w formie BOW tokenów liczbowych.\n",
    "\n",
    "Bez dodatkowego ważenia lub redukcji szumu model gubi się w tysiącach cech o podobnej wadze\n",
    "\n",
    "4.Naive Bayes nie radzi sobie dobrze z niektórymi wieloklasowymi problemami\n",
    "\n",
    "Model Naive Bayes dobrze działa w klasyfikacji binarnej lub przy bardzo wyraźnych różnicach między klasami.\n",
    "Przy subtelnych różnicach w języku (np. między recenzją na 3 a 4 gwiazdki) — potrzebowałby bardziej zaawansowanego modelu lub przynajmniej lepszej reprezentacji danych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eaa7e8",
   "metadata": {},
   "source": [
    "Ewaluacje danych dla 3 klas Pozytywna, Neutralna, Negatywna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1a35700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model i vectorizer zostały wczytane z jednego pliku.\n",
      "Wczytano dane testowe: 29999 rekordów.\n",
      "Rozkład klas w zbiorze testowym (przed mapowaniem):\n",
      "stars\n",
      "5    0.444867\n",
      "4    0.254133\n",
      "3    0.113767\n",
      "1    0.109133\n",
      "2    0.078100\n",
      "Name: proportion, dtype: float64\n",
      "Rozkład klas w zbiorze testowym (po mapowaniu):\n",
      "stars\n",
      "Pozytywna    0.699000\n",
      "Negatywna    0.187233\n",
      "Neutralna    0.113767\n",
      "Name: proportion, dtype: float64\n",
      "Dane testowe zostały przekształcone.\n",
      "Predykcja zakończona.\n",
      "Accuracy: 0.11457048568285609\n",
      "Precision: 0.6919181934156265\n",
      "Recall: 0.11457048568285609\n",
      "F1 Score: 0.02533184794256542\n",
      "Wyniki ewaluacji zapisane w folderze: ../evaluation/eval_BoW_70_30_PNN\n"
     ]
    }
   ],
   "source": [
    "# Ścieżki\n",
    "test_texts_path = '../data/70_30/test_texts_bow.csv'\n",
    "test_labels_path = '../data/70_30/test_labels.csv'\n",
    "model_path = '../models/BoW_70_30_PNN.pkl'\n",
    "evaluation_dir = '../evaluation/eval_BoW_70_30_PNN'\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "# Wczytujemy model i vectorizer z jednego pliku\n",
    "with open(model_path, 'rb') as file:\n",
    "    model_package = pickle.load(file)\n",
    "\n",
    "vectorizer = model_package['vectorizer']\n",
    "model = model_package['model']\n",
    "\n",
    "print(\"Model i vectorizer zostały wczytane z jednego pliku.\")\n",
    "\n",
    "# Wczytujemy dane testowe\n",
    "X_test_raw = pd.read_csv(test_texts_path, index_col=0)\n",
    "y_test = pd.read_csv(test_labels_path, index_col=0).squeeze()\n",
    "\n",
    "print(f\"Wczytano dane testowe: {X_test_raw.shape[0]} rekordów.\")\n",
    "\n",
    "# Rozkład klas w zbiorze testowym przed mapowaniem\n",
    "print(\"Rozkład klas w zbiorze testowym (przed mapowaniem):\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Mapujemy gwiazdki na klasy sentymentu\n",
    "def map_sentiment(star_rating):\n",
    "    if star_rating in [4, 5]:\n",
    "        return 'Pozytywna'\n",
    "    elif star_rating == 3:\n",
    "        return 'Neutralna'\n",
    "    else:\n",
    "        return 'Negatywna'\n",
    "\n",
    "y_test = y_test.map(map_sentiment)\n",
    "\n",
    "print(\"Rozkład klas w zbiorze testowym (po mapowaniu):\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Zapisujemy rozkład klas\n",
    "class_distribution = y_test.value_counts(normalize=True)\n",
    "class_distribution.to_csv(os.path.join(evaluation_dir, 'class_distribution_test_70_30.csv'))\n",
    "\n",
    "# Wyrównujemy indeksy\n",
    "X_test_raw, y_test = X_test_raw.align(y_test, join='inner', axis=0)\n",
    "\n",
    "# Parsujemy dane testowe\n",
    "X_test_dicts = X_test_raw.iloc[:, 0].apply(ast.literal_eval)\n",
    "\n",
    "# Transformujemy dane testowe\n",
    "X_test = vectorizer.transform(X_test_dicts)\n",
    "\n",
    "print(\"Dane testowe zostały przekształcone.\")\n",
    "\n",
    "# Predykcja\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Predykcja zakończona.\")\n",
    "\n",
    "# Obliczamy metryki\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=['Pozytywna', 'Neutralna', 'Negatywna'])\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Tworzymy podsumowanie metryk\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'metric': ['accuracy', 'precision', 'recall', 'f1_score'],\n",
    "    'value': [accuracy, precision, recall, f1]\n",
    "})\n",
    "\n",
    "# Zapisujemy podsumowanie do pliku CSV\n",
    "metrics_summary.to_csv(os.path.join(evaluation_dir, 'metrics_summary_70_30.csv'), index=False)\n",
    "\n",
    "# Confusion matrix zapisujemy osobno\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    conf_matrix,\n",
    "    index=['Pozytywna', 'Neutralna', 'Negatywna'],\n",
    "    columns=['Pozytywna', 'Neutralna', 'Negatywna']\n",
    ")\n",
    "conf_matrix_df.to_csv(os.path.join(evaluation_dir, 'confusion_matrix_70_30.csv'), index=True)\n",
    "\n",
    "print(f\"Wyniki ewaluacji zapisane w folderze: {evaluation_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be8e62fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model i vectorizer zostały wczytane z jednego pliku.\n",
      "Wczytano dane testowe: 19999 rekordów.\n",
      "Rozkład klas w zbiorze testowym (przed mapowaniem):\n",
      "stars\n",
      "5    0.44355\n",
      "4    0.25460\n",
      "3    0.11375\n",
      "1    0.10950\n",
      "2    0.07860\n",
      "Name: proportion, dtype: float64\n",
      "Rozkład klas w zbiorze testowym (po mapowaniu):\n",
      "stars\n",
      "Pozytywna    0.69815\n",
      "Negatywna    0.18810\n",
      "Neutralna    0.11375\n",
      "Name: proportion, dtype: float64\n",
      "Dane testowe zostały przekształcone.\n",
      "Predykcja zakończona.\n",
      "Accuracy: 0.11495574778738937\n",
      "Precision: 0.5586647699022541\n",
      "Recall: 0.11495574778738937\n",
      "F1 Score: 0.02655214176698218\n",
      "Wyniki ewaluacji zapisane w folderze: ../evaluation/eval_BoW_80_20_PNN\n"
     ]
    }
   ],
   "source": [
    "# Ścieżki\n",
    "test_texts_path = '../data/80_20/test_texts_bow.csv'\n",
    "test_labels_path = '../data/80_20/test_labels.csv'\n",
    "model_path = '../models/BoW_80_20_PNN.pkl'\n",
    "evaluation_dir = '../evaluation/eval_BoW_80_20_PNN'\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "# Wczytujemy model i vectorizer z jednego pliku\n",
    "with open(model_path, 'rb') as file:\n",
    "    model_package = pickle.load(file)\n",
    "\n",
    "vectorizer = model_package['vectorizer']\n",
    "model = model_package['model']\n",
    "\n",
    "print(\"Model i vectorizer zostały wczytane z jednego pliku.\")\n",
    "\n",
    "# Wczytujemy dane testowe\n",
    "X_test_raw = pd.read_csv(test_texts_path, index_col=0)\n",
    "y_test = pd.read_csv(test_labels_path, index_col=0).squeeze()\n",
    "\n",
    "print(f\"Wczytano dane testowe: {X_test_raw.shape[0]} rekordów.\")\n",
    "\n",
    "# Rozkład klas w zbiorze testowym przed mapowaniem\n",
    "print(\"Rozkład klas w zbiorze testowym (przed mapowaniem):\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Mapujemy gwiazdki na klasy sentymentu\n",
    "def map_sentiment(star_rating):\n",
    "    if star_rating in [4, 5]:\n",
    "        return 'Pozytywna'\n",
    "    elif star_rating == 3:\n",
    "        return 'Neutralna'\n",
    "    else:\n",
    "        return 'Negatywna'\n",
    "\n",
    "y_test = y_test.map(map_sentiment)\n",
    "\n",
    "print(\"Rozkład klas w zbiorze testowym (po mapowaniu):\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Zapisujemy rozkład klas\n",
    "class_distribution = y_test.value_counts(normalize=True)\n",
    "class_distribution.to_csv(os.path.join(evaluation_dir, 'class_distribution_test_80_20.csv'))\n",
    "\n",
    "# Wyrównujemy indeksy\n",
    "X_test_raw, y_test = X_test_raw.align(y_test, join='inner', axis=0)\n",
    "\n",
    "# Parsujemy dane testowe\n",
    "X_test_dicts = X_test_raw.iloc[:, 0].apply(ast.literal_eval)\n",
    "\n",
    "# Transformujemy dane testowe\n",
    "X_test = vectorizer.transform(X_test_dicts)\n",
    "\n",
    "print(\"Dane testowe zostały przekształcone.\")\n",
    "\n",
    "# Predykcja\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Predykcja zakończona.\")\n",
    "\n",
    "# Obliczamy metryki\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=['Pozytywna', 'Neutralna', 'Negatywna'])\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Tworzymy podsumowanie metryk\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'metric': ['accuracy', 'precision', 'recall', 'f1_score'],\n",
    "    'value': [accuracy, precision, recall, f1]\n",
    "})\n",
    "\n",
    "# Zapisujemy podsumowanie do pliku CSV\n",
    "metrics_summary.to_csv(os.path.join(evaluation_dir, 'metrics_summary_80_20.csv'), index=False)\n",
    "\n",
    "# Confusion matrix zapisujemy osobno\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    conf_matrix,\n",
    "    index=['Pozytywna', 'Neutralna', 'Negatywna'],\n",
    "    columns=['Pozytywna', 'Neutralna', 'Negatywna']\n",
    ")\n",
    "conf_matrix_df.to_csv(os.path.join(evaluation_dir, 'confusion_matrix_80_20.csv'), index=True)\n",
    "\n",
    "print(f\"Wyniki ewaluacji zapisane w folderze: {evaluation_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e08556",
   "metadata": {},
   "source": [
    "Interpretacja:\n",
    "\n",
    "    Rozkład klas jest OK — Pozytywna jest dominująca, jak się spodziewaliśmy, około 70%.\n",
    "\n",
    "    Accuracy ~11% — dość niskie.\n",
    "    To naturalne przy takim podstawowym modelu (Naive Bayes + Bag of Words). Powód:\n",
    "\n",
    "    Pozytywna klasa jest dominująca, a model nie rozpoznaje jej wystarczająco dobrze.\n",
    "\n",
    "    Naive Bayes + BoW nie radzi sobie z subtelnymi różnicami w opiniach użytkowników.\n",
    "\n",
    "    Precision jest wysoka: ~69% — to znaczy, że jeśli model już coś przewidzi jako pozytywne, to ma raczej rację.\n",
    "    To typowe przy dominującej klasie i nieco słabszym recallu.\n",
    "\n",
    "    Recall nisko (~11%) — oznacza, że model pomija dużą część rzeczywistych pozytywnych przykładów, przewiduje je jako inne klasy lub nie rozpoznaje ich dobrze.\n",
    "\n",
    "    F1 Score niski (~2.5%) — bo F1 to kompromis między precision i recall, a tutaj recall jest nisko."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
