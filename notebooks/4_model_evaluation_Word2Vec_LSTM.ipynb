{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9712a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importujemy biblioteki\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114443af",
   "metadata": {},
   "source": [
    "Ewaluacja danych 70_30 dla PNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2892924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model został wczytany.\n",
      "LabelEncoder został wczytany.\n",
      "Wczytano dane testowe: 29999 rekordów.\n",
      "Rozkład klas w zbiorze testowym (przed mapowaniem):\n",
      "stars\n",
      "5    0.444867\n",
      "4    0.254133\n",
      "3    0.113767\n",
      "1    0.109133\n",
      "2    0.078100\n",
      "Name: proportion, dtype: float64\n",
      "Rozkład klas w zbiorze testowym (po mapowaniu):\n",
      "stars\n",
      "Pozytywna    0.699000\n",
      "Negatywna    0.187233\n",
      "Neutralna    0.113767\n",
      "Name: proportion, dtype: float64\n",
      "Kształt danych testowych po paddingu: (29999, 500, 50)\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 32ms/step\n",
      "Predykcja zakończona.\n",
      "Accuracy: 0.7076902563418781\n",
      "Precision: 0.6482241652049239\n",
      "Recall: 0.7076902563418781\n",
      "F1 Score: 0.6144421466195277\n",
      "Wyniki ewaluacji zapisane w folderze: ../evaluation/eval_w2v_LSTM_70_30_PNN\n"
     ]
    }
   ],
   "source": [
    "# Ścieżki\n",
    "test_texts_path = '../data/70_30/test_texts_w2v.csv'\n",
    "test_labels_path = '../data/70_30/test_labels.csv'\n",
    "model_path = '../models/w2v_LSTM_70_30_PNN.h5'\n",
    "label_encoder_path = '../models/w2v_LSTM_70_30_PNN_label_encoder.pkl'\n",
    "evaluation_dir = '../evaluation/eval_w2v_LSTM_70_30_PNN'\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "# Wczytujemy model\n",
    "model = load_model(model_path)\n",
    "\n",
    "print(\"Model został wczytany.\")\n",
    "\n",
    "# Wczytujemy label encoder\n",
    "with open(label_encoder_path, 'rb') as le_file:\n",
    "    label_encoder = pickle.load(le_file)\n",
    "\n",
    "print(\"LabelEncoder został wczytany.\")\n",
    "\n",
    "# Wczytujemy dane testowe\n",
    "X_test_raw = pd.read_csv(test_texts_path, index_col=0)\n",
    "y_test = pd.read_csv(test_labels_path, index_col=0).squeeze()\n",
    "\n",
    "print(f\"Wczytano dane testowe: {X_test_raw.shape[0]} rekordów.\")\n",
    "\n",
    "# Rozkład klas w zbiorze testowym (przed mapowaniem)\n",
    "print(\"Rozkład klas w zbiorze testowym (przed mapowaniem):\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Mapujemy gwiazdki na klasy sentymentu\n",
    "def map_sentiment(star_rating):\n",
    "    if star_rating in [4, 5]:\n",
    "        return 'Pozytywna'\n",
    "    elif star_rating == 3:\n",
    "        return 'Neutralna'                         \n",
    "    else:\n",
    "        return 'Negatywna'\n",
    "\n",
    "y_test = y_test.map(map_sentiment)\n",
    "\n",
    "print(\"Rozkład klas w zbiorze testowym (po mapowaniu):\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Zapisujemy rozkład klas\n",
    "class_distribution = y_test.value_counts(normalize=True)\n",
    "class_distribution.to_csv(os.path.join(evaluation_dir, 'class_distribution_test_70_30.csv'))\n",
    "\n",
    "# Wyrównujemy indeksy\n",
    "X_test_raw, y_test = X_test_raw.align(y_test, join='inner', axis=0)\n",
    "\n",
    "# Parsujemy dane testowe\n",
    "X_test_list = X_test_raw['text'].apply(ast.literal_eval).apply(np.array).tolist()\n",
    "\n",
    "# Padding sekwencji\n",
    "X_test_padded = pad_sequences(X_test_list, padding='post', dtype='float32')\n",
    "\n",
    "print(f\"Kształt danych testowych po paddingu: {X_test_padded.shape}\")\n",
    "\n",
    "# Predykcja\n",
    "y_pred_proba = model.predict(X_test_padded)\n",
    "y_pred = y_pred_proba.argmax(axis=1)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Predykcja zakończona.\")\n",
    "\n",
    "# Obliczamy metryki\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "precision = precision_score(y_test, y_pred_labels, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred_labels, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred_labels, average='weighted', zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_labels, labels=label_encoder.classes_)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Tworzymy podsumowanie metryk\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'metric': ['accuracy', 'precision', 'recall', 'f1_score'],\n",
    "    'value': [accuracy, precision, recall, f1]\n",
    "})\n",
    "\n",
    "# Zapisujemy podsumowanie do pliku CSV\n",
    "metrics_summary.to_csv(os.path.join(evaluation_dir, 'metrics_summary_70_30.csv'), index=False)\n",
    "\n",
    "# Confusion matrix zapisujemy osobno\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    conf_matrix,\n",
    "    index=label_encoder.classes_,\n",
    "    columns=label_encoder.classes_\n",
    ")\n",
    "conf_matrix_df.to_csv(os.path.join(evaluation_dir, 'confusion_matrix_70_30.csv'), index=True)\n",
    "\n",
    "print(f\"Wyniki ewaluacji zapisane w folderze: {evaluation_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b17fea",
   "metadata": {},
   "source": [
    "Ewaluacja danych 80_20 dla PNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a6eea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model został wczytany.\n",
      "LabelEncoder został wczytany.\n",
      "Wczytano dane testowe: 19999 rekordów.\n",
      "Rozkład klas w zbiorze testowym (przed mapowaniem):\n",
      "stars\n",
      "5    0.44355\n",
      "4    0.25460\n",
      "3    0.11375\n",
      "1    0.10950\n",
      "2    0.07860\n",
      "Name: proportion, dtype: float64\n",
      "Rozkład klas w zbiorze testowym (po mapowaniu):\n",
      "stars\n",
      "Pozytywna    0.69815\n",
      "Negatywna    0.18810\n",
      "Neutralna    0.11375\n",
      "Name: proportion, dtype: float64\n",
      "Kształt danych testowych po paddingu: (19999, 475, 50)\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 28ms/step\n",
      "Predykcja zakończona.\n",
      "Accuracy: 0.6734836741837091\n",
      "Precision: 0.5143140518510328\n",
      "Recall: 0.6734836741837091\n",
      "F1 Score: 0.5658320449173618\n",
      "Wyniki ewaluacji zapisane w folderze: ../evaluation/eval_w2v_LSTM_80_20_PNN\n"
     ]
    }
   ],
   "source": [
    "# Ścieżki\n",
    "test_texts_path = '../data/80_20/test_texts_w2v.csv'\n",
    "test_labels_path = '../data/80_20/test_labels.csv'\n",
    "model_path = '../models/w2v_LSTM_80_20_PNN.h5'\n",
    "label_encoder_path = '../models/w2v_LSTM_80_20_PNN_label_encoder.pkl'\n",
    "evaluation_dir = '../evaluation/eval_w2v_LSTM_80_20_PNN'\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "# Wczytujemy model\n",
    "model = load_model(model_path)\n",
    "\n",
    "print(\"Model został wczytany.\")\n",
    "\n",
    "# Wczytujemy label encoder\n",
    "with open(label_encoder_path, 'rb') as le_file:\n",
    "    label_encoder = pickle.load(le_file)\n",
    "\n",
    "print(\"LabelEncoder został wczytany.\")\n",
    "\n",
    "# Wczytujemy dane testowe\n",
    "X_test_raw = pd.read_csv(test_texts_path, index_col=0)\n",
    "y_test = pd.read_csv(test_labels_path, index_col=0).squeeze()\n",
    "\n",
    "print(f\"Wczytano dane testowe: {X_test_raw.shape[0]} rekordów.\")\n",
    "\n",
    "# Rozkład klas w zbiorze testowym (przed mapowaniem)\n",
    "print(\"Rozkład klas w zbiorze testowym (przed mapowaniem):\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Mapujemy gwiazdki na klasy sentymentu\n",
    "def map_sentiment(star_rating):\n",
    "    if star_rating in [4, 5]:\n",
    "        return 'Pozytywna'\n",
    "    elif star_rating == 3:\n",
    "        return 'Neutralna'\n",
    "    else:\n",
    "        return 'Negatywna'\n",
    "\n",
    "y_test = y_test.map(map_sentiment)\n",
    "\n",
    "print(\"Rozkład klas w zbiorze testowym (po mapowaniu):\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Zapisujemy rozkład klas\n",
    "class_distribution = y_test.value_counts(normalize=True)\n",
    "class_distribution.to_csv(os.path.join(evaluation_dir, 'class_distribution_test_80_20.csv'))\n",
    "\n",
    "# Wyrównujemy indeksy\n",
    "X_test_raw, y_test = X_test_raw.align(y_test, join='inner', axis=0)\n",
    "\n",
    "# Parsujemy dane testowe\n",
    "X_test_list = X_test_raw['text'].apply(ast.literal_eval).apply(np.array).tolist()\n",
    "\n",
    "# Padding sekwencji\n",
    "X_test_padded = pad_sequences(X_test_list, padding='post', dtype='float32')\n",
    "\n",
    "print(f\"Kształt danych testowych po paddingu: {X_test_padded.shape}\")\n",
    "\n",
    "# Predykcja\n",
    "y_pred_proba = model.predict(X_test_padded)\n",
    "y_pred = y_pred_proba.argmax(axis=1)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Predykcja zakończona.\")\n",
    "\n",
    "# Obliczamy metryki\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "precision = precision_score(y_test, y_pred_labels, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred_labels, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred_labels, average='weighted', zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_labels, labels=label_encoder.classes_)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Tworzymy podsumowanie metryk\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'metric': ['accuracy', 'precision', 'recall', 'f1_score'],\n",
    "    'value': [accuracy, precision, recall, f1]\n",
    "})\n",
    "\n",
    "# Zapisujemy podsumowanie do pliku CSV\n",
    "metrics_summary.to_csv(os.path.join(evaluation_dir, 'metrics_summary_80_20.csv'), index=False)\n",
    "\n",
    "# Confusion matrix zapisujemy osobno\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    conf_matrix,\n",
    "    index=label_encoder.classes_,\n",
    "    columns=label_encoder.classes_\n",
    ")\n",
    "conf_matrix_df.to_csv(os.path.join(evaluation_dir, 'confusion_matrix_80_20.csv'), index=True)\n",
    "\n",
    "print(f\"Wyniki ewaluacji zapisane w folderze: {evaluation_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded3d0a7",
   "metadata": {},
   "source": [
    "ewaluacja danych 70-30 5 gwiazdek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fedbda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model został wczytany.\n",
      "LabelEncoder został wczytany.\n",
      "Wczytano dane testowe: 29999 rekordów.\n",
      "Rozkład klas w zbiorze testowym (gwiazdki):\n",
      "stars\n",
      "5    0.444867\n",
      "4    0.254133\n",
      "3    0.113767\n",
      "1    0.109133\n",
      "2    0.078100\n",
      "Name: proportion, dtype: float64\n",
      "Zapisano rozkład klas do pliku.\n",
      "Po align: X_test_raw shape: (29999, 1), y_test shape: (29999,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x0000024569E6DD80>>\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Maciej\\Desktop\\sem6\\UCZENIE MASZYNOWE\\sentiment_analysis\\.venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# Ścieżki\n",
    "test_texts_path = '../data/70_30/test_texts_w2v.csv'\n",
    "test_labels_path = '../data/70_30/test_labels.csv'\n",
    "model_path = '../models/w2v_LSTM_70_30_stars.h5'\n",
    "label_encoder_path = '../models/w2v_LSTM_70_30_stars_label_encoder.pkl'\n",
    "evaluation_dir = '../evaluation/eval_w2v_LSTM_70_30_stars'\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "# Wczytujemy model\n",
    "model = load_model(model_path)\n",
    "print(\"Model został wczytany.\")\n",
    "\n",
    "# Wczytujemy label encoder\n",
    "with open(label_encoder_path, 'rb') as le_file:\n",
    "    label_encoder = pickle.load(le_file)\n",
    "print(\"LabelEncoder został wczytany.\")\n",
    "\n",
    "# Wczytujemy dane testowe\n",
    "X_test_raw = pd.read_csv(test_texts_path, index_col=0)\n",
    "y_test = pd.read_csv(test_labels_path, index_col=0).squeeze()\n",
    "print(f\"Wczytano dane testowe: {X_test_raw.shape[0]} rekordów.\")\n",
    "\n",
    "# Rozkład klas w zbiorze testowym\n",
    "print(\"Rozkład klas w zbiorze testowym (gwiazdki):\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Zapisujemy rozkład klas\n",
    "class_distribution = y_test.value_counts(normalize=True)\n",
    "class_distribution.to_csv(os.path.join(evaluation_dir, 'class_distribution_test_70_30.csv'))\n",
    "print(\"Zapisano rozkład klas do pliku.\")\n",
    "\n",
    "# Wyrównujemy indeksy\n",
    "X_test_raw, y_test = X_test_raw.align(y_test, join='inner', axis=0)\n",
    "print(f\"Po align: X_test_raw shape: {X_test_raw.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Parsowanie danych testowych\n",
    "X_test_list = X_test_raw['text'].apply(ast.literal_eval).apply(np.array).tolist()\n",
    "\n",
    "# Padding sekwencji\n",
    "X_test_padded = pad_sequences(X_test_list, padding='post', dtype='float32')\n",
    "print(f\"Kształt danych testowych po paddingu: {X_test_padded.shape}\")\n",
    "\n",
    "# Predykcja\n",
    "y_pred_proba = model.predict(X_test_padded)\n",
    "y_pred = y_pred_proba.argmax(axis=1)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Predykcja zakończona.\")\n",
    "\n",
    "# Rozkład klas w predykcji\n",
    "print(\"Rozkład klas w predykcji (gwiazdki):\")\n",
    "print(pd.Series(y_pred_labels).value_counts(normalize=True))\n",
    "\n",
    "# Obliczamy metryki\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "precision = precision_score(y_test, y_pred_labels, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred_labels, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred_labels, average='weighted', zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_labels, labels=label_encoder.classes_)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Tworzymy podsumowanie metryk\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'metric': ['accuracy', 'precision', 'recall', 'f1_score'],\n",
    "    'value': [accuracy, precision, recall, f1]\n",
    "})\n",
    "\n",
    "# Zapisujemy podsumowanie do pliku CSV\n",
    "metrics_summary.to_csv(os.path.join(evaluation_dir, 'metrics_summary_70_30.csv'), index=False)\n",
    "print(\"Zapisano metrics_summary_70_30.csv.\")\n",
    "\n",
    "# Confusion matrix zapisujemy osobno\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    conf_matrix,\n",
    "    index=label_encoder.classes_,\n",
    "    columns=label_encoder.classes_\n",
    ")\n",
    "conf_matrix_df.to_csv(os.path.join(evaluation_dir, 'confusion_matrix_70_30.csv'), index=True)\n",
    "print(\"Zapisano confusion_matrix_70_30.csv.\")\n",
    "\n",
    "print(f\"Wyniki ewaluacji zapisane w folderze: {evaluation_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a765b142",
   "metadata": {},
   "source": [
    "ewaluacja danych 80-20 5 gwiazdek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f40021c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model został wczytany.\n",
      "LabelEncoder został wczytany.\n",
      "Wczytano dane testowe: 19999 rekordów.\n",
      "Rozkład klas w zbiorze testowym (gwiazdki):\n",
      "stars\n",
      "5    0.44355\n",
      "4    0.25460\n",
      "3    0.11375\n",
      "1    0.10950\n",
      "2    0.07860\n",
      "Name: proportion, dtype: float64\n",
      "Zapisano rozkład klas do pliku.\n",
      "Po align: X_test_raw shape: (19999, 1), y_test shape: (19999,)\n",
      "Kształt danych testowych po paddingu: (19999, 475, 50)\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 29ms/step\n",
      "Predykcja zakończona.\n",
      "Rozkład klas w predykcji (gwiazdki):\n",
      "5    0.809990\n",
      "4    0.142707\n",
      "1    0.026551\n",
      "3    0.016851\n",
      "2    0.003900\n",
      "Name: proportion, dtype: float64\n",
      "Accuracy: 0.3962\n",
      "Precision: 0.2856\n",
      "Recall: 0.3962\n",
      "F1 Score: 0.3069\n",
      "Zapisano metrics_summary_80_20.csv.\n",
      "Zapisano confusion_matrix_80_20.csv.\n",
      "Wyniki ewaluacji zapisane w folderze: ../evaluation/eval_w2v_LSTM_80_20_stars\n"
     ]
    }
   ],
   "source": [
    "# Ścieżki\n",
    "test_texts_path = '../data/80_20/test_texts_w2v.csv'\n",
    "test_labels_path = '../data/80_20/test_labels.csv'\n",
    "model_path = '../models/w2v_LSTM_80_20_stars.h5'\n",
    "label_encoder_path = '../models/w2v_LSTM_80_20_stars_label_encoder.pkl'\n",
    "evaluation_dir = '../evaluation/eval_w2v_LSTM_80_20_stars'\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "# Wczytujemy model\n",
    "model = load_model(model_path)\n",
    "print(\"Model został wczytany.\")\n",
    "\n",
    "# Wczytujemy label encoder\n",
    "with open(label_encoder_path, 'rb') as le_file:\n",
    "    label_encoder = pickle.load(le_file)\n",
    "print(\"LabelEncoder został wczytany.\")\n",
    "\n",
    "# Wczytujemy dane testowe\n",
    "X_test_raw = pd.read_csv(test_texts_path, index_col=0)\n",
    "y_test = pd.read_csv(test_labels_path, index_col=0).squeeze()\n",
    "print(f\"Wczytano dane testowe: {X_test_raw.shape[0]} rekordów.\")\n",
    "\n",
    "# Rozkład klas w zbiorze testowym\n",
    "print(\"Rozkład klas w zbiorze testowym (gwiazdki):\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Zapisujemy rozkład klas\n",
    "class_distribution = y_test.value_counts(normalize=True)\n",
    "class_distribution.to_csv(os.path.join(evaluation_dir, 'class_distribution_test_80_20.csv'))\n",
    "print(\"Zapisano rozkład klas do pliku.\")\n",
    "\n",
    "# Wyrównujemy indeksy\n",
    "X_test_raw, y_test = X_test_raw.align(y_test, join='inner', axis=0)\n",
    "print(f\"Po align: X_test_raw shape: {X_test_raw.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Parsowanie danych testowych\n",
    "X_test_list = X_test_raw['text'].apply(ast.literal_eval).apply(np.array).tolist()\n",
    "\n",
    "# Padding sekwencji\n",
    "X_test_padded = pad_sequences(X_test_list, padding='post', dtype='float32')\n",
    "print(f\"Kształt danych testowych po paddingu: {X_test_padded.shape}\")\n",
    "\n",
    "# Predykcja\n",
    "y_pred_proba = model.predict(X_test_padded)\n",
    "y_pred = y_pred_proba.argmax(axis=1)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "print(\"Predykcja zakończona.\")\n",
    "\n",
    "# Rozkład klas w predykcji\n",
    "print(\"Rozkład klas w predykcji (gwiazdki):\")\n",
    "print(pd.Series(y_pred_labels).value_counts(normalize=True))\n",
    "\n",
    "# Obliczamy metryki\n",
    "accuracy = accuracy_score(y_test, y_pred_labels)\n",
    "precision = precision_score(y_test, y_pred_labels, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred_labels, average='weighted', zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred_labels, average='weighted', zero_division=0)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_labels, labels=label_encoder.classes_)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Tworzymy podsumowanie metryk\n",
    "metrics_summary = pd.DataFrame({\n",
    "    'metric': ['accuracy', 'precision', 'recall', 'f1_score'],\n",
    "    'value': [accuracy, precision, recall, f1]\n",
    "})\n",
    "\n",
    "# Zapisujemy podsumowanie do pliku CSV\n",
    "metrics_summary.to_csv(os.path.join(evaluation_dir, 'metrics_summary_80_20.csv'), index=False)\n",
    "print(\"Zapisano metrics_summary_80_20.csv.\")\n",
    "\n",
    "# Confusion matrix zapisujemy osobno\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    conf_matrix,\n",
    "    index=label_encoder.classes_,\n",
    "    columns=label_encoder.classes_\n",
    ")\n",
    "conf_matrix_df.to_csv(os.path.join(evaluation_dir, 'confusion_matrix_80_20.csv'), index=True)\n",
    "print(\"Zapisano confusion_matrix_80_20.csv.\")\n",
    "\n",
    "print(f\"Wyniki ewaluacji zapisane w folderze: {evaluation_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
